{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !pip install tensorflow-gpu\n",
    "# !pip install torch\n",
    "# !pip torchvision\n",
    "# !pip transformers\n",
    "# !pip install timm\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install ipykernel\n",
    "\n",
    "# !pip install jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook\n",
    "# !pip install --upgrade numpy\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow transformers torch torchvision timm pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaBRBTmjHNyH",
    "outputId": "20e7021a-59c9-453b-bdf5-fa94397e1131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2025-04-02 21:06:51.509279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install required libraries\n",
    "# !pip install torch torchvision transformers timm pandas scikit-learn\n",
    "\n",
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0pofeBTRAFO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "i5VuUl8vN98i"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom ImageFolder to skip empty directories\n",
    "class SafeImageFolder(datasets.ImageFolder):\n",
    "    @staticmethod\n",
    "    def make_dataset(\n",
    "        directory, class_to_idx, extensions=None, is_valid_file=None, allow_empty=False\n",
    "    ):\n",
    "        instances = []\n",
    "        directory = os.path.expanduser(directory)\n",
    "\n",
    "        for target_class in sorted(class_to_idx.keys()):\n",
    "            target_dir = os.path.join(directory, target_class)\n",
    "            if not os.path.isdir(target_dir):\n",
    "                continue\n",
    "\n",
    "            # Check if directory has valid files\n",
    "            has_files = any(\n",
    "                datasets.folder.has_file_allowed_extension(os.path.join(root, fname), extensions)\n",
    "                for root, _, fnames in os.walk(target_dir, followlinks=True)\n",
    "                for fname in fnames\n",
    "            )\n",
    "\n",
    "            if not has_files and not allow_empty:\n",
    "                continue  # Skip empty directory if allow_empty is False\n",
    "\n",
    "            # Collect valid files\n",
    "            for root, _, fnames in os.walk(target_dir, followlinks=True):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    if (is_valid_file or datasets.folder.has_file_allowed_extension(path, extensions)):\n",
    "                        item = (path, class_to_idx[target_class])\n",
    "                        instances.append(item)\n",
    "        return instances\n",
    "\n",
    "# ImageNet Mean and Std\n",
    "IMAGENET_DEFAULT_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_DEFAULT_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transformations\n",
    "img_size = 224\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)  # âœ… Corrected\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)  # âœ… Corrected\n",
    "])\n",
    "\n",
    "# Load datasets with SafeImageFolder\n",
    "data_dir = '../dataset/all-data'\n",
    "train_dataset = SafeImageFolder(root=f'{data_dir}/train', transform=train_transform)\n",
    "val_dataset = SafeImageFolder(root=f'{data_dir}/val', transform=val_transform)\n",
    "test_dataset = SafeImageFolder(root=f'{data_dir}/test', transform=val_transform)\n",
    "\n",
    "# Create data loaders (fixed batch_size typo)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)  # âœ… Fixed b_size -> batch_size\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)  # âœ… Fixed b_size -> batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "d391cf167fc5475aaa77282703c6623c"
     ]
    },
    "id": "I-SVX8KuOAwW",
    "outputId": "2e7588ed-d0bc-41ed-eaa9-ad1266b08bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class EfficientNetForAnimalRecognition(nn.Module):\n",
    "    def __init__(self, num_classes=1072):\n",
    "        super().__init__()\n",
    "        # Load pre-trained EfficientNet-B3 from timm\n",
    "        self.efficientnet = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)  # Removes classifier\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "        self.classifier = nn.Linear(1536, num_classes)  # EfficientNet-B3 has 1536-dim feature embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.efficientnet.forward_features(x)  # Extract feature maps\n",
    "        features = self.global_pool(features)  # Apply Global Average Pooling\n",
    "        features = torch.flatten(features, 1)  # Flatten to (batch_size, 1536)\n",
    "        return self.classifier(features)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extracts feature embeddings from the EfficientNet backbone (without classification layer).\"\"\"\n",
    "        features = self.efficientnet.forward_features(x)\n",
    "        features = self.global_pool(features)\n",
    "        return torch.flatten(features, 1)\n",
    "\n",
    "# Detect available device (CUDA for Colab, MPS for Mac)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize and move model to the device\n",
    "model = EfficientNetForAnimalRecognition(num_classes=1072).to(device)\n",
    "\n",
    "# If using multiple GPUs in Colab, enable DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "krTKdhK1OFuV"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ztu4iM1cOJ4x",
    "outputId": "f079e88a-3336-4a30-c848-d57b0d233602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 6.6814 | Val Loss: 6.1669 | Val Acc: 0.0634\n",
      "Epoch 2/30\n",
      "Train Loss: 5.6436 | Val Loss: 4.8681 | Val Acc: 0.2255\n",
      "Epoch 3/30\n",
      "Train Loss: 4.6144 | Val Loss: 3.8564 | Val Acc: 0.4074\n",
      "Epoch 4/30\n",
      "Train Loss: 3.9014 | Val Loss: 3.0511 | Val Acc: 0.5317\n",
      "Epoch 5/30\n",
      "Train Loss: 3.3781 | Val Loss: 2.6681 | Val Acc: 0.5849\n",
      "Epoch 6/30\n",
      "Train Loss: 2.9688 | Val Loss: 2.2406 | Val Acc: 0.6329\n",
      "Epoch 7/30\n",
      "Train Loss: 2.6020 | Val Loss: 1.9289 | Val Acc: 0.6682\n",
      "Epoch 8/30\n",
      "Train Loss: 2.2705 | Val Loss: 1.6178 | Val Acc: 0.7181\n",
      "Epoch 9/30\n",
      "Train Loss: 1.9766 | Val Loss: 1.3791 | Val Acc: 0.7630\n",
      "Epoch 10/30\n",
      "Train Loss: 1.7017 | Val Loss: 1.1182 | Val Acc: 0.8053\n",
      "Epoch 12/30\n",
      "Train Loss: 1.2577 | Val Loss: 0.7592 | Val Acc: 0.8469\n",
      "Epoch 13/30\n",
      "Train Loss: 1.0720 | Val Loss: 0.6466 | Val Acc: 0.8533\n",
      "Epoch 14/30\n",
      "Train Loss: 0.9122 | Val Loss: 0.5443 | Val Acc: 0.8700\n",
      "Epoch 15/30\n",
      "Train Loss: 0.8054 | Val Loss: 0.4646 | Val Acc: 0.8757\n",
      "Epoch 16/30\n",
      "Train Loss: 0.7071 | Val Loss: 0.4253 | Val Acc: 0.8770\n",
      "Epoch 17/30\n",
      "Train Loss: 0.6364 | Val Loss: 0.3781 | Val Acc: 0.8873\n",
      "Epoch 18/30\n",
      "Train Loss: 0.5630 | Val Loss: 0.3333 | Val Acc: 0.8911\n",
      "Epoch 19/30\n",
      "Train Loss: 0.5222 | Val Loss: 0.3102 | Val Acc: 0.8834\n",
      "Epoch 20/30\n",
      "Train Loss: 0.4805 | Val Loss: 0.2931 | Val Acc: 0.8873\n",
      "Epoch 21/30\n",
      "Train Loss: 0.4465 | Val Loss: 0.2770 | Val Acc: 0.8898\n",
      "Epoch 22/30\n",
      "Train Loss: 0.4122 | Val Loss: 0.2703 | Val Acc: 0.8834\n",
      "Epoch 23/30\n",
      "Train Loss: 0.4064 | Val Loss: 0.2734 | Val Acc: 0.8885\n",
      "Epoch 24/30\n",
      "Train Loss: 0.4008 | Val Loss: 0.2680 | Val Acc: 0.8847\n",
      "Epoch 25/30\n",
      "Train Loss: 0.4034 | Val Loss: 0.2721 | Val Acc: 0.8885\n",
      "Epoch 26/30\n",
      "Train Loss: 0.3880 | Val Loss: 0.2657 | Val Acc: 0.8879\n",
      "Epoch 27/30\n",
      "Train Loss: 0.3936 | Val Loss: 0.2667 | Val Acc: 0.8905\n",
      "Epoch 28/30\n",
      "Train Loss: 0.3941 | Val Loss: 0.2746 | Val Acc: 0.8905\n",
      "Epoch 29/30\n",
      "Train Loss: 0.3982 | Val Loss: 0.2711 | Val Acc: 0.8905\n",
      "Epoch 30/30\n",
      "Train Loss: 0.4044 | Val Loss: 0.2671 | Val Acc: 0.8930\n",
      "Best Validation Accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, num_epochs=10):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()  # âœ… Ensure labels are of type torch.long\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # âœ… Debugging: Check label range\n",
    "            if labels.max() >= outputs.shape[1] or labels.min() < 0:\n",
    "                print(f\"ðŸ”¥ Label out of range! Max: {labels.max()}, Min: {labels.min()}\")\n",
    "                print(f\"Expected range: 0 to {outputs.shape[1]-1}\")\n",
    "                return\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.long()  # âœ… Ensure labels are torch.long\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = correct / total\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), './best_model.pth')\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_acc:.4f}')\n",
    "\n",
    "# âœ… Add CUDA debugging environment variable (optional, for debugging)\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "train_model(model, num_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8DK5e65aOJ_n"
   },
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            features_batch = model.extract_features(inputs)\n",
    "            features.append(features_batch.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "\n",
    "    return np.concatenate(features), np.concatenate(labels_list)\n",
    "\n",
    "# Extract features from training data\n",
    "train_features, train_labels = extract_features(model, train_loader)\n",
    "\n",
    "# Save features\n",
    "np.save('./train_features.npy', train_features)\n",
    "np.save('./train_labels.npy', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "C7igteU0ONZg",
    "outputId": "7c6d2704-1477-4b96-ce96-2ee97d4dbd31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {correct / total:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('./best_model.pth', weights_only=True))\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad049WA_lbjd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
